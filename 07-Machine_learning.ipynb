{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import chemiscope\n",
    "from widget_code_input import WidgetCodeInput\n",
    "from ipywidgets import Textarea\n",
    "from iam_utils import *\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "from ase.calculators import lj, eam\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### AVOID folding of output cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:5000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dump = WidgetDataDumper(prefix=\"module_07\")\n",
    "display(data_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_summary = Textarea(\"general comments on this module\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"module-summary\", module_summary, \"value\")\n",
    "display(module_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reference textbook: ???? _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a very brief and over-simplified primer on \"data-driven\" modeling. \n",
    "In abstract terms, a data-driven approach attempts to establish a relationship between _input_ data and _target_ properties (or to recognize patterns in the data itself) without using deductive reasoning, i.e. without proceeding though a series of logical steps starting from an hypothesis on the physical behavior of a system. \n",
    "\n",
    "Instead, the empirical association between inputs and targets is taken as the only basis to establish an _inductive_ relationship between them. The traditional scientific method proceeds through a combination of induction and deduction, while data-driven approaches are intended to be entirely inductive. On the risks of purely inductive reasoning, see [Bertrand Russel's inductivist chicken story](http://www.ditext.com/russell/rus6.html). In practice, _inductive biases_ are often included in the modeling, through by means of the choices that are made in the construction and the tuning of the model itself: this is how a component of physics-inspired (deductive) concepts can make it back into machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the most primitive example of data-driven modeling, consider the case of _linear regression_. \n",
    "A set of $n_\\mathrm{train}$ data points and targets $\\{x_i, y_i\\} $ are assumed to follow a linear relationship of the form $y(x)=a x$, where the slope $a$ is an adjustable parameter. \n",
    "For a given value of $a$, one can compute the _loss_, i.e. the root mean square error between the true value of the targets and the predictions of the model,\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i |y(x_i)-y_i|^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This widget allows you to play around with the core idea of linear regression: by adjusting the value of $a$ you can minimize the discrepancy between predictions and targets, and find the best model within the class chosen to represent the input-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "lr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "lr_y = 2.33*lr_x+(np.random.uniform(size=20)-0.5)*2\n",
    "def lr_plot(ax, a):    \n",
    "    ax.plot(lr_x, lr_y, 'b.')\n",
    "    ax.plot([-5,5],[-5*a,5*a], 'r--')\n",
    "    l = np.mean((lr_y-a*lr_x)**2)\n",
    "    ax.text(-4,8,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "wp_lr = WidgetPlot(lr_plot, WidgetParbox(a=(1.0, -5.0, 5.0, 0.1, r'$a$')))\n",
    "display(wp_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**01** What is (roughly) the best value of $a$ that minimizes the loss in the linear regression model? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex1-answer\", ex1_txt, \"value\")\n",
    "display(ex1_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, the loss can be minimized with a closed expression: given that $\\partial \\ell/\\partial a \\propto \\sum_i x_i(a x_i -y_i)$, it's easy to see that an extremum occurs for $a = \\sum_i x_i y_i / \\sum_i x_i^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be easily generalized to more complex models: in the most general terms, $\\ell$ can be minimized numerically, by computing the derivatives of $y(x)$ with respect to the model parameters. \n",
    "Here we consider the simpler case of a polynomial model, in which $y(x)=\\sum_k w_k x^k$. This can actually be seen as a special case of multi-dimensional linear regression, where each sample is described by several _features_ (or _descriptors_), in this case $x_{ik}=x_i^k$. \n",
    "\n",
    "_NB: this is a very bad choice of a polynomial basis to expand the function (most notably, because the different polynomials are not orthogonal). We are just doing this as a simple example, never try this for a real problem!_\n",
    "\n",
    "Play around with the widget below. It is _really_ difficult to fit the model manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "np.random.seed(12345)\n",
    "pr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "pr_y = (np.random.uniform(size=20)-0.5)*3\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "for k in range(len(pr_w)):\n",
    "    pr_y += pr_w[k]*pr_x**k\n",
    "    \n",
    "def pr_plot(ax, **w):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    lw = list(w.values())\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    fit_w = np.linalg.lstsq(pr_X, pr_y, rcond=None)[0]\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    for k in range(len(lw)):\n",
    "        yy += lw[k]*xx**k\n",
    "        my += lw[k]*pr_x**k\n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        \n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-my)**2)\n",
    "    ax.plot(pr_x, pr_y, 'b.', label=\"train data\")\n",
    "    ax.plot(xx, yy, 'r--', label=\"manual fit\")\n",
    "    ax.text(-4,-1,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_ylim(min(pr_y)-1, max(pr_y)+1)\n",
    "wp_pr = WidgetPlot(pr_plot, WidgetParbox(\n",
    "    w_0=(1.0, -5.0, 5.0, 0.01,  r'$w_0$'),\n",
    "    w_1=(0.01, -2.0, 2.0, 0.01, r'$w_1$'),\n",
    "    w_2=(0.01, -1.0, 1.0, 0.01, r'$w_2$'),\n",
    "    w_3=(-0.2, -1.0, 1.0, 0.01, r'$w_3$'),\n",
    "    w_4=(0.01, -0.1, 0.1, 0.01, r'$w_4$'),\n",
    "    w_5=(0.01, -0.1, 0.1, 0.01, r'$w_5$')\n",
    "))\n",
    "display(wp_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can be written in a vectorial form, $\\ell \\propto \\sum_i | \\mathbf{w}\\cdot\\mathbf{x}_i - y_i |^2$. If $\\mathbf{X}$ is the matrix collecting the $x_i^k$ as rows and $\\mathbf{y}$ is the vector collecting the targets, a closed form solution for the weight vector can be derived as\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "We can now start looking to more realistic issues that arise in the context of regression models. For starters, data can contain a certain level of _noise_. This can be actual noise, or (often) hidden input features or relationships that cannot be captured by the simplified model. Second, a model that predicts the targets only for the data it had been trained on is of very little use: we want to be able to do real predictions!\n",
    "For this reason, it is customary to set aside a fraction of the available data that is not used to determine the weights that minimize $\\ell$. The error on this _test set_ is an indication of how good the model would be when predicting a new point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "def pr2_plot(ax, tgt, fit, noise, hidden, npoints):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(12345)\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*10\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    \n",
    "    fit_w = np.linalg.lstsq(pr_X[::2], pr_y[::2], rcond=None)[0]    \n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(pr_y)-1, max(pr_y)+1)    \n",
    "    ax.text(0.1,0.05,f'$\\ell_\\mathrm{{train}} = ${l:.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.15,f'$\\ell_\\mathrm{{test}} = ${lte:.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "wp_pr2 = WidgetPlot(pr2_plot, WidgetParbox(    \n",
    "    noise=(5.0, 0.1,10,0.1, 'Noise'),\n",
    "    hidden=(0.0, 0, 5,0.01, 'Hidden', {\"readout_format\" : \".2f\"}),\n",
    "    npoints=(15, 10, 100, 1, r'$n_\\mathrm{train}$'),\n",
    "    tgt=(False, r'Show true target'),\n",
    "    fit=(True, r'Show best fit'),\n",
    "))\n",
    "display(wp_pr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**02** Compare the error on the train and the test sets. Which is typically higher? How do train and test errors change when the number of training points is changed from the lowest to the highest level?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2a_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2a-answer\", ex2a_txt, \"value\")\n",
    "display(ex2a_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**02b** How do the train and test loss change when the level of noise is increased? And how do they change when the level of hidden relationships is increased? Can you clearly distinguish the effect of noise and hidden terms? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2b_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2b-answer\", ex2b_txt, \"value\")\n",
    "display(ex2b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprints and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "461px",
    "left": "0px",
    "right": "927.667px",
    "top": "107px",
    "width": "139px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
