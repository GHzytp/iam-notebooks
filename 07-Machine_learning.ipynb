{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import chemiscope\n",
    "from widget_code_input import WidgetCodeInput\n",
    "from ipywidgets import Textarea\n",
    "from iam_utils import *\n",
    "import ase\n",
    "import functools\n",
    "import copy\n",
    "from ase.io import read, write\n",
    "from ase.calculators import lj, eam\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### AVOID folding of output cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:5000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dump = WidgetDataDumper(prefix=\"module_07\")\n",
    "display(data_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_summary = Textarea(\"general comments on this module\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"module-summary\", module_summary, \"value\")\n",
    "display(module_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References: [Nature 559, 547â€“555 (2018)](https://www.nature.com/articles/s41586-018-0337-2)\n",
    "[J. Chem. Phys. 150, 150901 (2019)](https://doi.org/10.1063/1.5091842)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data-driven modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This module provides a very brief and over-simplified primer on \"data-driven\" modeling. \n",
    "In abstract terms, a data-driven approach attempts to establish a relationship between _input_ data and _target_ properties (or to recognize patterns in the data itself) without using deductive reasoning, i.e. without proceeding though a series of logical steps starting from an hypothesis on the physical behavior of a system. \n",
    "\n",
    "Instead, the empirical association between inputs and targets is taken as the only basis to establish an _inductive_ relationship between them. The traditional scientific method proceeds through a combination of induction and deduction, while data-driven approaches are intended to be entirely inductive. On the risks of purely inductive reasoning, see [Bertrand Russel's inductivist chicken story](http://www.ditext.com/russell/rus6.html). In practice, _inductive biases_ are often included in the modeling, through by means of the choices that are made in the construction and the tuning of the model itself: this is how a component of physics-inspired (deductive) concepts can make it back into machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As the most primitive example of data-driven modeling, consider the case of _linear regression_. \n",
    "A set of $n_\\mathrm{train}$ data points and targets $\\{x_i, y_i\\} $ are assumed to follow a linear relationship of the form $y(x)=a x$, where the slope $a$ is an adjustable parameter. \n",
    "For a given value of $a$, one can compute the _loss_, i.e. the root mean square error between the true value of the targets and the predictions of the model,\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i |y(x_i)-y_i|^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This widget allows you to play around with the core idea of linear regression: by adjusting the value of $a$ you can minimize the discrepancy between predictions and targets, and find the best model within the class chosen to represent the input-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "lr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "lr_y = 2.33*lr_x+(np.random.uniform(size=20)-0.5)*2\n",
    "def lr_plot(ax, a):    \n",
    "    ax.plot(lr_x, lr_y, 'b.')\n",
    "    ax.plot([-5,5],[-5*a,5*a], 'r--')\n",
    "    l = np.mean((lr_y-a*lr_x)**2)\n",
    "    ax.text(-4,8,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "wp_lr = WidgetPlot(lr_plot, WidgetParbox(a=(1.0, -5.0, 5.0, 0.1, r'$a$')))\n",
    "display(wp_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**01** What is (roughly) the best value of $a$ that minimizes the loss in the linear regression model? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex1_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex1-answer\", ex1_txt, \"value\")\n",
    "display(ex1_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a linear regression model, the loss can be minimized with a closed expression: given that $\\partial \\ell/\\partial a \\propto \\sum_i x_i(a x_i -y_i)$, it's easy to see that an extremum occurs for $a = \\sum_i x_i y_i / \\sum_i x_i^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This approach can be easily generalized to more complex models: in the most general terms, $\\ell$ can be minimized numerically, by computing the derivatives of $y(x)$ with respect to the model parameters. \n",
    "Here we consider the simpler case of a polynomial model, in which $y(x)=\\sum_k w_k x^k$. This can actually be seen as a special case of multi-dimensional linear regression, where each sample is described by several _features_ (or _descriptors_), in this case $x_{ik}=x_i^k$. \n",
    "\n",
    "_NB: this is a very bad choice of a polynomial basis to expand the function (most notably, because the different polynomials are not orthogonal). We are just doing this as a simple example, never try this for a real problem!_\n",
    "\n",
    "Play around with the widget below. It is _really_ difficult to fit the model manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "np.random.seed(12345)\n",
    "pr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "pr_y = (np.random.uniform(size=20)-0.5)*3\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "for k in range(len(pr_w)):\n",
    "    pr_y += pr_w[k]*pr_x**k\n",
    "    \n",
    "def pr_plot(ax, **w):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    lw = list(w.values())\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    fit_w = np.linalg.lstsq(pr_X, pr_y, rcond=None)[0]\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    for k in range(len(lw)):\n",
    "        yy += lw[k]*xx**k\n",
    "        my += lw[k]*pr_x**k\n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        \n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-my)**2)\n",
    "    ax.plot(pr_x, pr_y, 'b.', label=\"train data\")\n",
    "    ax.plot(xx, yy, 'r--', label=\"manual fit\")\n",
    "    ax.text(-4,-1,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_ylim(min(pr_y)-1, max(pr_y)+1)\n",
    "wp_pr = WidgetPlot(pr_plot, WidgetParbox(\n",
    "    w_0=(1.0, -5.0, 5.0, 0.01,  r'$w_0$'),\n",
    "    w_1=(0.01, -2.0, 2.0, 0.01, r'$w_1$'),\n",
    "    w_2=(0.01, -1.0, 1.0, 0.01, r'$w_2$'),\n",
    "    w_3=(-0.2, -1.0, 1.0, 0.01, r'$w_3$'),\n",
    "    w_4=(0.01, -0.1, 0.1, 0.01, r'$w_4$'),\n",
    "    w_5=(0.01, -0.1, 0.1, 0.01, r'$w_5$')\n",
    "))\n",
    "display(wp_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loss can be written in a vectorial form, $\\ell \\propto \\sum_i | \\mathbf{w}\\cdot\\mathbf{x}_i - y_i |^2$. If $\\mathbf{X}$ is the matrix collecting the $x_i^k$ as rows and $\\mathbf{y}$ is the vector collecting the targets, a closed form solution for the weight vector can be derived as\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "We can now start looking to more realistic issues that arise in the context of regression models. For starters, data can contain a certain level of _noise_. This can be actual noise, or (often) hidden input features or relationships that cannot be captured by the simplified model. Second, a model that predicts the targets only for the data it had been trained on is of very little use: we want to be able to do real predictions!\n",
    "For this reason, it is customary to set aside a fraction of the available data that is not used to determine the weights that minimize $\\ell$. The error on this _test set_ is an indication of how good the model would be when predicting a new point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "def pr2_plot(ax, tgt, fit, noise, hidden, npoints):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(12345)\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*10\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    \n",
    "    fit_w = np.linalg.lstsq(pr_X[::2], pr_y[::2], rcond=None)[0]    \n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(ty)-1, max(ty)+1+noise/2)    \n",
    "    ax.text(0.1,0.15,f'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.05,f'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "wp_pr2 = WidgetPlot(pr2_plot, WidgetParbox(    \n",
    "    noise=(5.0, 0.1,10,0.1, 'Noise'),\n",
    "    hidden=(0.0, 0, 5,0.01, 'Hidden', {\"readout_format\" : \".2f\"}),\n",
    "    npoints=(20, 5, 100, 1, r'$n_\\mathrm{train}$'),\n",
    "    tgt=(False, r'Show true target'),\n",
    "    fit=(True, r'Show best fit'),\n",
    "))\n",
    "display(wp_pr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**02a** Compare the error on the train and the test sets. Which is typically higher? How do train and test errors change when the number of training points is changed from the lowest to the highest level?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex2a_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2a-answer\", ex2a_txt, \"value\")\n",
    "display(ex2a_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**02b** How do the train and test loss change when the level of noise is increased? And how do they change when the level of hidden relationships is increased? Can you clearly distinguish the effect of noise and hidden terms? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex2b_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2b-answer\", ex2b_txt, \"value\")\n",
    "display(ex2b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The tendency of achieving very low loss on the train set and a much larger test set error is a general phenomenon known as [_overfitting_](https://en.wikipedia.org/wiki/Overfitting). Overfitting is usually particularly bad when the train set size is small, and/or the model contains many parameters. Polynomial regression is notorious for overfitting. \n",
    "\n",
    "A common strategy to avoid overfitting is known as _regularization_. In broad terms, regularization implies penalizing solutions of the model that are too rapidly varying, and favouring those that are smoother, even at the cost of a slight increase of the train set error. In linear regression, the most common approach is to introduce a [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) term, that is to write the loss as \n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i | \\mathbf{w}\\cdot \\mathbf{x}_i - y_i | ^2 + \\lambda |\\mathbf{w}|^2.\n",
    "$$\n",
    "\n",
    "This expression (often referred to as $L^2$ regularized least-squares fit, or ridge regression) yields a closed-form solution for the weights, \n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X}+\\lambda \\mathbf{1})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This widget allows you to experiment with the effect of ridge regularization on the same polynomial fitting exercise. \n",
    "\n",
    "_NB: given that we are using a very poor basis, and different features span widely different scales, the underlying implementation is slightly more complicated, in that different weights are scaled differently before computing the Tikhonov term. This scaling is done so that a single parameter can be meaningfully used to control the regularity of the fit._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "def pr3_plot(ax, tgt, fit, noise, hidden, npoints, lam):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(54321)\n",
    "    xsz = 10\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*xsz\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    wscale = np.asarray([1/(xsz*0.5)**k for k in range(npoly+1)])\n",
    "    fit_w = np.linalg.solve(\n",
    "        pr_X[::2].T@pr_X[::2]+\n",
    "        10**lam*(npoints//2)*np.diag(wscale**2), \n",
    "        pr_X[::2].T@pr_y[::2])\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(ty)-1, max(ty)+1+noise/2)    \n",
    "    ax.text(0.1,0.25,f'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.15,f'$|\\mathbf{{w}}| = ${np.linalg.norm(wscale*fit_w):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.05,f'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "wp_pr3 = WidgetPlot(pr3_plot, WidgetParbox(    \n",
    "    noise=(5.0, 0.1,10,0.1, 'Noise'),\n",
    "    hidden=(0.0, 0, 5,0.01, 'Hidden', {\"readout_format\" : \".2f\"}),\n",
    "    npoints=(10, 5, 100, 1, r'$n_\\mathrm{train}$'),\n",
    "    tgt=(True, r'Show true target'),\n",
    "    fit=(True, r'Show best fit'),\n",
    "    lam = (-5.0,-5,5,0.1, r'$\\log_{10} \\lambda$')\n",
    "))\n",
    "display(wp_pr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**03a** Work with (noise, hidden, ntrain) = (5,0,10). What is the value of $\\lambda$ that minimizes the _test_ error?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex3a_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3a-answer\", ex3a_txt, \"value\")\n",
    "display(ex3a_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**03b** Working with the same number of parameters, comment on the behavior of the best fit function and the various diagnostics as you vary the regularization away from the optimum value.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex3b_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3b-answer\", ex3b_txt, \"value\")\n",
    "display(ex3b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:blue\">**03c** Increase the number of training points to 100. How does the behavior of ridge regression change? Is the same value of $\\lambda$ still optimal? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex3c_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3c-answer\", ex3c_txt, \"value\")\n",
    "display(ex3c_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The regularization $\\lambda$ is one of the so-called _hyperparameters_ (\"hypers\"), that tune the behavior of the model but are not directly optimized on the train set. In this case, the number of polynomial terms is another hyperparameter. Optimizing the hyperparameters on the test set is bad practice, because this amounts to _data leakage_, and makes the test error less representative of the true generalization capabilities of the model. \n",
    "\n",
    "We won't get into details, but consider that strategies to optimize the hyperparameters is to set aside a _validation_ set that is not used for training nor for testing, but just to tune the hyperparameter values, or to perform _cross validation_, that implies splitting the training set into train/validation parts, and repeating the exercise over multiple _folds_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprints and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any data-driven study of materials involves codifying the structure and composition of the materials being studied into a mathematical form that is suitable to be used as the input of the subsequent steps. Here we focus in particular on the definition of _fingerprints_, or _descriptors_ - a vector of numbers that are associated with each structure, assembled into a _feature vector_ $\\mathbf{x}_i$. \n",
    "\n",
    "In this module we are going to use a dataset of materials from the [materials project](https://materialsproject.org/). The dataset has been reformatted as an extended XYZ file, that can be read, as usual, with the ASE `read` function. The target properties (mostly related to the elastic behavior) can be read in the `info` dictionary of each frame.\n",
    "\n",
    "```\n",
    "data = read(\"filename.xyz\", \":\")\n",
    "X = []\n",
    "y = []\n",
    "for structure in data:\n",
    "    X.append(get_features(structure.symbols, structure.cell, structure.positions))\n",
    "    y.append(structure.info['property_name'])\n",
    "```\n",
    "\n",
    "where `get_features` is a function that processes the structural information of each frame to convert it into a set of fingerprints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptors can be either precise [_representations_](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00021) of the coordinates and chemical nature of all atoms in a structure (which are commonplace in the construction of machine-learning interatomic potentials) or fingerprints based on a combination of structural parameters and properties that can be computed easily. For instance, one could take the electronegativity of elements in combination with the point group of the crystal structure. \n",
    "\n",
    "Here we take a very simple (and somewhat crude) approach, describing each structure by its chemical composition - that is, the feature $x_{iZ}$ contains the fraction of the atoms of structure $i$ that has atomic number $Z$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**04** Write a function that takes structural information for each frame and returns a vector containing the fractional composition of each compound, to be used as descriptors. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex01_wci = WidgetCodeInput(\n",
    "        function_name=\"descriptor_base\", \n",
    "        function_parameters=\"structure_file\",\n",
    "        docstring=\"\"\"loads the file given in `structure_file` and computes a descriptor \n",
    "        which is a vector of the fractional composition of each structure in the dataset, \n",
    "        e.g. given [H2, He2, HHe, LiH] returns something like \n",
    "        [[1,0,0],[0,1,0],[0.5,0.5,0],[0.5,0,0.5]]. \n",
    "        The total vector size depends on how many elements are present in the data set,\n",
    "        but it's OK if there are zeros. \n",
    "\"\"\",\n",
    "        function_body=\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from ase.io import read\n",
    "\n",
    "structures = read(structure_file, ':')\n",
    "descriptor = np.zeros((len(structures), 100))\n",
    "for i in range(len(structures)):\n",
    "    for z in structures[i].numbers:\n",
    "        descriptor[i,z] += 1\n",
    "    descriptor[i]/=len(structures[i])\n",
    "    \n",
    "return descriptor\n",
    "\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "data_dump.register_field(\"ex01-function\", ex01_wci, \"function_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_html_table(numpy_array, header):\n",
    "    rows = \"\"\n",
    "    for i in range(len(numpy_array)):\n",
    "        rows += f\"<tr><td>{numpy_array[i][0]}</td>\" + functools.reduce(lambda x,y: x+y,\n",
    "                             map(lambda x: f\"<td>{x:.2f}</td>\",\n",
    "                                 numpy_array[i][1:])\n",
    "                            ) + \"</tr>\"\n",
    "\n",
    "    return \"<table>\" + header + rows + \"</table>\"\n",
    "\n",
    "def mk_table():\n",
    "    structures=read('data/mp_elastic.extxyz',':')\n",
    "    l = ex01_wci.get_function_object()('data/mp_elastic.extxyz')\n",
    "\n",
    "    x = []   \n",
    "    for a,b in enumerate(l):\n",
    "        s = structures[a].symbols\n",
    "        x.append([s]+list(b))\n",
    "\n",
    "    header = \"\"\"<tr>\n",
    "                  <th>Symbols <span style=\"padding-left:150px\"></th>\n",
    "                  <th>Fractional composition upto order n <span style=\"padding-left:150px\"></th>\n",
    "                </tr>\"\"\"\n",
    "    demo_table_html = HTML(\n",
    "        value=f\"Table\")\n",
    "    demo_table_html.value = array_to_html_table(x[::100], header)\n",
    "\n",
    "    demo_table = HBox(layout=Layout(height='250px', overflow_y='auto'))\n",
    "    demo_table.children += (demo_table_html,) \n",
    "    display(demo_table)\n",
    "    \n",
    "feat_table = WidgetUpdater(mk_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divya todo: make the table look prettier, with some spacing between columns and the first line spanning multiple columns\n",
    "ex01_wcc = WidgetCodeCheck(ex01_wci, ref_values = {}, demo = feat_table)\n",
    "display(ex01_wcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVYA: get them to write a function that loads the structures, computes the vectors with the fractional composition.\n",
    "# it should take as an argument the maximum polynomial order, and also compute the powers of the fractional composition,\n",
    "# stacking them to form a larger feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex02_wci = WidgetCodeInput(\n",
    "        function_name=\"descriptor_poly\", \n",
    "        function_parameters=\"structure_file, nmax\",\n",
    "        docstring=\"\"\"compute the powers of the fractional composition and stack them \n",
    "        to form a larger feature vector\n",
    "        \n",
    "        nmax : maximum order of the polynomial\n",
    "\"\"\",\n",
    "        function_body=\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from ase.io import read\n",
    "\n",
    "\n",
    "structures = read(structure_file, ':')\n",
    "descriptor = np.zeros((len(structures), 100))\n",
    "for i in range(len(structures)):\n",
    "    for z in structures[i].numbers:\n",
    "        descriptor[i,z] += 1\n",
    "    descriptor[i]/=len(structures[i])\n",
    "\n",
    "descriptor = np.hstack([descriptor**k for k in range(1,nmax+1)])\n",
    "\n",
    "return descriptor\n",
    "\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "data_dump.register_field(\"ex02-function\", ex02_wci, \"function_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mk_table2():\n",
    "    structures=read('data/mp_elastic.extxyz',':')\n",
    "    l = ex02_wci.get_function_object()('data/mp_elastic.extxyz', ex02_wp.value['n'])\n",
    "\n",
    "    x = []   \n",
    "    for a,b in enumerate(l):\n",
    "        s = structures[a].symbols\n",
    "        x.append([s]+list(b))\n",
    "\n",
    "    header = \"\"\"<tr>\n",
    "                  <th>Symbols <span style=\"padding-left:150px\"></th>\n",
    "                  <th>Fractional composition upto order n <span style=\"padding-left:150px\"></th>\n",
    "                </tr>\"\"\"\n",
    "    demo_table_html = HTML(\n",
    "        value=f\"Table\")\n",
    "    demo_table_html.value = array_to_html_table(x[::100], header)\n",
    "\n",
    "    demo_table = HBox(layout=Layout(height='250px', overflow_y='auto'))\n",
    "    demo_table.children += (demo_table_html,) \n",
    "    display(demo_table)\n",
    "ex02_wp =  WidgetParbox(n = (2,1,10,1,r'$n_{max}$'))    \n",
    "feat_table2 = WidgetUpdater(mk_table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex02_wcc = WidgetCodeCheck(ex02_wci, ref_values = {}, demo=VBox([ex02_wp,feat_table2]))\n",
    "display(ex02_wcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a scaffold for a \"custom\" function where they can play around and compute their own fingerprints based on the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a quick example of principal components analysis (PCA) one of the simplest _unsupervised_ learning algorithms - actually, one that can hardly be called machine learning. \n",
    "PCA involves processing a high-dimensional feature matrix $\\mathbf{X}$ and projecting it _linearly_ into a _latent space_ $\\mathbf{T}$ of reduced dimensionality. The problem can be formulated in different terms: as the identification of the directions with maximum variance in feature space, as the maximisation of the variance retained in the latent space or as the low-rank orthogonal projection of the feature matrix that minimizes the information loss. \n",
    "\n",
    "The figure below demonstrates the functioning of PCA on a simple 2D dataset: the principal axes of the data distribution are identified, making it possible to reduce the description to just 1D while losing the smallest possible amount of information on the relative position of the points\n",
    "\n",
    "<img src=\"figures/pca.png\" width=\"500\"/>\n",
    "\n",
    "In rigorous terms, PCA corresponds to determine the orthogonal projection matrix $\\mathbf{P}_{{XT}}$ that minimizes the loss\n",
    "\n",
    "$$\n",
    "\\ell = |\\mathbf{X}-\\mathbf{X}\\mathbf{P}_{XT} \\mathbf{P}_{XT}^T|^2\n",
    "$$\n",
    "\n",
    "To understand what this does, consider that $\\mathbf{P}_{{XT}}$ is a $n_X\\times n_T$ matrix; $\\mathbf{X}\\mathbf{P}_{XT}$ transforms the feature vector of each point into a $n_T$-dimensional compressed version, forming a latent-space matrix $\\mathbf{T}$. \n",
    "Then $\\mathbf{T}\\mathbf{P}_{XT}^T$ lifts this to the $n_X$-dimensional space. Given the compression, however, information has been lost and the data lie in a subspace (the blue points in the figure above). \n",
    "\n",
    "The projector $\\mathbf{P}_{XT}$ can be built using a [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $\\mathbf{X}$ or, equivalently, by computing the eigenvalue decomposition of the covariance matrix $\\mathbf{X}^T\\mathbf{X}$. Note that usually the feature matrix is _centered_ before identifying the principal components, that is the column-wise average of the features is subtracted from each row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's really easy to implement PCA manually, it is even simpler to use one of the many open implementations available, that also take care of centering. We use the implementation in `scikit-learn`. You are encouraged to read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), but the key workflow is simple:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)   # n_components: dimension of the latent space\n",
    "itrain = range(0,ntrain)  # list of indices used for training\n",
    "pca.fit(x[itrain])      # x[itrain] is a ntrain x nx feature matrix\n",
    "t = pca.transform(x)    # applies compression to the full feature vector. \n",
    "                        # t is nsamples x n_components \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO divya: make them code up a PCA analysis, \n",
    "#using half of the structures and predicting on the other half, and showing the result in a chemiscope.\n",
    "# use sklearn.decomposition.\n",
    "# the function signature should take the filename and the function to be called - \n",
    "# that's the only way to access what they wrote in the previous exercises.\n",
    "# it should return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex03_wci = WidgetCodeInput(\n",
    "        function_name=\"PCA_analysis\", \n",
    "        function_parameters=\"structure_file, function_name, nmax\",\n",
    "        docstring=\"\"\"takes the filename and the function name that you wrote in the above exercise and performs a PCA\n",
    "        analysis on the descriptor to reduce its dimensionality\n",
    "\n",
    "\"\"\",\n",
    "        function_body=\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from ase.io import read\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "structures = read(structure_file, ':')\n",
    "\n",
    "X = function_name(structure_file, nmax)\n",
    "#Split the dataset into train and test set\n",
    "X_train = X[::2]\n",
    "X_test =  X[1::2]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "return X_pca\n",
    "\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "data_dump.register_field(\"ex03-function\", ex03_wci, \"function_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_ex03(change={'type':'change'}):\n",
    "    ex03_out.clear_output()\n",
    "    fun_inputs = copy.deepcopy(ex03_wp.value)\n",
    "    \n",
    "    with ex03_out:\n",
    "        descriptor_poly = ex02_wci.get_function_object()\n",
    "        fname = ex03_wci.get_function_object()('data/mp_elastic.extxyz', descriptor_poly, ex03_wp.value['n'])\n",
    "    \n",
    "    structures = read('data/mp_elastic.extxyz',':')\n",
    "    frames=structures\n",
    "    \n",
    "    properties={\"pca[1]\": fname[:,0], \"pca[2]\" : fname[:,1], \"type\":([\"train\",\"test\"]*590)[:-1]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    chemiscope.write_input(\"module_07-pca-analysis.chemiscoÃ¥pe.json.gz\", frames=frames, properties=properties)\n",
    "    with ex03_up:\n",
    "        display(chemiscope.show(frames, properties,\n",
    "                               settings={'map': {'x': {'property': 'pca[1]'},\n",
    "  'y': { 'property': 'pca[2]'},\n",
    "  #'color': {'max': 1, 'min': 0, 'property': 'K_error', 'scale': 'linear'},\n",
    "  'symbol': 'type',\n",
    "  'palette': 'inferno',\n",
    "  'size': {'factor': 40}},\n",
    " 'structure': [{'bonds': True,\n",
    "   'spaceFilling': False,\n",
    "   'atomLabels': False,\n",
    "   'unitCell': True,\n",
    "   'rotation': False,\n",
    "   'supercell': {'0': 2, '1': 2, '2': 2},}]}\n",
    "                               ))\n",
    "    \n",
    "ex03_up = WidgetUpdater(updater=fun_ex03)\n",
    "ex03_out = Output(layout=Layout(width='100%', height='100%', max_height='200px', overflow_y='scroll'))\n",
    "ex03_wp =  WidgetParbox(\n",
    "    n = (2,1,10,1,r'$n_{max}$'))    \n",
    "\n",
    "\n",
    "ex03_wcc = WidgetCodeCheck(ex03_wci, ref_values = {}, demo = (ex03_wp, ex03_out, ex03_up))\n",
    "\n",
    "display(ex03_wcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ex02_wci.get_function_object()('data/mp_elastic.extxyz', 3)\n",
    "s = read('data/mp_elastic.extxyz',':')\n",
    "y = np.asarray([f.info['K'] for f in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(x[::2])\n",
    "t = pca.transform(x[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(frames=s[1::2], properties={\"pca[1]\": t[:,0], \"pca[2]\" : t[:,1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO divya: make them code up a ridge regression module\n",
    "#using half of the structures and predicting on the other half, and showing the result in a chemiscope.\n",
    "# use sklearn.linear_model.Ridge.\n",
    "# alpha should be an adjustable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex04_wci = WidgetCodeInput(\n",
    "        function_name=\"ridge_regression\", \n",
    "        function_parameters=\"structure_file, function_name, nmax, alpha\",\n",
    "        docstring=\"\"\"takes the filename and the function name that you wrote in the above exercise and performs a ridge\n",
    "        regression on the dataset\n",
    "        \n",
    "        nmax: maximum order of the parameter\n",
    "        alpha: Regularization parameter\n",
    "\"\"\",\n",
    "        function_body=\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from ase.io import read\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "structures = read(structure_file, ':')\n",
    "\n",
    "X = function_name(structure_file, nmax)\n",
    "#Split the dataset into train and test set\n",
    "X_train = X[::2]\n",
    "X_test =  X[1::2]\n",
    "\n",
    "Y = np.asarray([f.info['K'] for f in structures])\n",
    "Y_train = Y[::2]\n",
    "Y_test =  Y[1::2]\n",
    "\n",
    "ridge = Ridge(alpha)\n",
    "ridge.fit(X_train, Y_train)\n",
    "Y_pred = ridge.predict(X) \n",
    "\n",
    "return Y_pred\n",
    "\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "data_dump.register_field(\"ex04-function\", ex04_wci, \"function_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_ex04(change={'type':'change'}):\n",
    "    ex04_out.clear_output()\n",
    "    fun_inputs = copy.deepcopy(ex04_wp.value)\n",
    "    \n",
    "    with ex04_out:\n",
    "        descriptor_poly = ex02_wci.get_function_object()\n",
    "        fname = ex04_wci.get_function_object()('data/mp_elastic.extxyz', descriptor_poly, ex04_wp.value['n'], 10**ex04_wp.value['log10alpha'])\n",
    "    \n",
    "    structures = read('data/mp_elastic.extxyz',':')\n",
    "    frames=structures\n",
    "    properties={\"K\": y, \"K_predicted\" : yp, \"K_error\": y-yp,\n",
    "                               \"type\":([\"train\",\"test\"]*590)[:-1]}\n",
    "    \n",
    "    \n",
    "    chemiscope.write_input(\"module_07-ridge-regression.chemiscoÃ¥pe.json.gz\", frames=frames, properties=properties)\n",
    "                           \n",
    "    with ex04_up:\n",
    "        display(chemiscope.show(frames=structures, \n",
    "                   properties={\"K\": y, \"K_predicted\" : yp, \"K_error\": y-yp,\n",
    "                               \"type\":([\"train\",\"test\"]*590)[:-1]},\n",
    "                  settings={'map': {'x': {'property': 'K'},\n",
    "  'y': { 'property': 'K_predicted'},\n",
    "  'color': {'max': 1, 'min': 0, 'property': 'K_error', 'scale': 'linear'},\n",
    "  'symbol': 'type',\n",
    "  'palette': 'inferno',\n",
    "  'size': {'factor': 40}},\n",
    " 'structure': [{'bonds': True,\n",
    "   'spaceFilling': False,\n",
    "   'atomLabels': False,\n",
    "   'unitCell': True,\n",
    "   'rotation': False,\n",
    "   'supercell': {'0': 2, '1': 2, '2': 2},}]}\n",
    "                  ))\n",
    "        #display(chemiscope.show(frames, properties, settings))\n",
    "    \n",
    "ex04_up = WidgetUpdater(updater=fun_ex04)\n",
    "ex04_out = Output(layout=Layout(width='100%', height='100%', max_height='200px', overflow_y='scroll'))\n",
    "ex04_wp =  WidgetParbox(n = (2,1,10,1,r'$n_{max}$'), \n",
    "                        log10alpha=(-3, -5, 0, 1, r\"$\\log_{10}(\\alpha)$\")\n",
    "                       )    \n",
    "\n",
    "\n",
    "ex04_wcc = WidgetCodeCheck(ex04_wci, ref_values = {}, demo = (ex04_wp, ex04_out, ex04_up))\n",
    "\n",
    "display(ex04_wcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ex02_wci.get_function_object()('data/mp_elastic.extxyz', 1)\n",
    "s = read('data/mp_elastic.extxyz',':')\n",
    "y = np.asarray([f.info['K'] for f in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=1e-3)\n",
    "ridge.fit(x[::2], y[::2])\n",
    "yp = ridge.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=chemiscope.show(frames=s, \n",
    "                   properties={\"K\": y, \"K_predicted\" : yp, \"K_error\": y-yp,\n",
    "                               \"type\":([\"train\",\"test\"]*590)[:-1]},\n",
    "                  settings={'map': {'x': {'property': 'K'},\n",
    "  'y': { 'property': 'K_predicted'},\n",
    "  'color': {'max': 1, 'min': 0, 'property': 'K_error', 'scale': 'linear'},\n",
    "  'symbol': 'type',\n",
    "  'palette': 'inferno',\n",
    "  'size': {'factor': 40}},\n",
    " 'structure': [{'bonds': True,\n",
    "   'spaceFilling': False,\n",
    "   'atomLabels': False,\n",
    "   'unitCell': True,\n",
    "   'rotation': False,\n",
    "   'supercell': {'0': 2, '1': 2, '2': 2},}]}\n",
    "                  )\n",
    "display(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "461px",
    "left": "0px",
    "right": "927.667px",
    "top": "107px",
    "width": "139px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
