{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import chemiscope\n",
    "from widget_code_input import WidgetCodeInput\n",
    "from ipywidgets import Textarea\n",
    "from iam_utils import *\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "from ase.calculators import lj, eam\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### AVOID folding of output cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:5000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dump = WidgetDataDumper(prefix=\"module_07\")\n",
    "display(data_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_summary = Textarea(\"general comments on this module\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"module-summary\", module_summary, \"value\")\n",
    "display(module_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References: [Nature 559, 547â€“555 (2018)](https://www.nature.com/articles/s41586-018-0337-2)\n",
    "[J. Chem. Phys. 150, 150901 (2019)](https://doi.org/10.1063/1.5091842)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a very brief and over-simplified primer on \"data-driven\" modeling. \n",
    "In abstract terms, a data-driven approach attempts to establish a relationship between _input_ data and _target_ properties (or to recognize patterns in the data itself) without using deductive reasoning, i.e. without proceeding though a series of logical steps starting from an hypothesis on the physical behavior of a system. \n",
    "\n",
    "Instead, the empirical association between inputs and targets is taken as the only basis to establish an _inductive_ relationship between them. The traditional scientific method proceeds through a combination of induction and deduction, while data-driven approaches are intended to be entirely inductive. On the risks of purely inductive reasoning, see [Bertrand Russel's inductivist chicken story](http://www.ditext.com/russell/rus6.html). In practice, _inductive biases_ are often included in the modeling, through by means of the choices that are made in the construction and the tuning of the model itself: this is how a component of physics-inspired (deductive) concepts can make it back into machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the most primitive example of data-driven modeling, consider the case of _linear regression_. \n",
    "A set of $n_\\mathrm{train}$ data points and targets $\\{x_i, y_i\\} $ are assumed to follow a linear relationship of the form $y(x)=a x$, where the slope $a$ is an adjustable parameter. \n",
    "For a given value of $a$, one can compute the _loss_, i.e. the root mean square error between the true value of the targets and the predictions of the model,\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i |y(x_i)-y_i|^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This widget allows you to play around with the core idea of linear regression: by adjusting the value of $a$ you can minimize the discrepancy between predictions and targets, and find the best model within the class chosen to represent the input-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "lr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "lr_y = 2.33*lr_x+(np.random.uniform(size=20)-0.5)*2\n",
    "def lr_plot(ax, a):    \n",
    "    ax.plot(lr_x, lr_y, 'b.')\n",
    "    ax.plot([-5,5],[-5*a,5*a], 'r--')\n",
    "    l = np.mean((lr_y-a*lr_x)**2)\n",
    "    ax.text(-4,8,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "wp_lr = WidgetPlot(lr_plot, WidgetParbox(a=(1.0, -5.0, 5.0, 0.1, r'$a$')))\n",
    "display(wp_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**01** What is (roughly) the best value of $a$ that minimizes the loss in the linear regression model? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex1-answer\", ex1_txt, \"value\")\n",
    "display(ex1_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, the loss can be minimized with a closed expression: given that $\\partial \\ell/\\partial a \\propto \\sum_i x_i(a x_i -y_i)$, it's easy to see that an extremum occurs for $a = \\sum_i x_i y_i / \\sum_i x_i^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be easily generalized to more complex models: in the most general terms, $\\ell$ can be minimized numerically, by computing the derivatives of $y(x)$ with respect to the model parameters. \n",
    "Here we consider the simpler case of a polynomial model, in which $y(x)=\\sum_k w_k x^k$. This can actually be seen as a special case of multi-dimensional linear regression, where each sample is described by several _features_ (or _descriptors_), in this case $x_{ik}=x_i^k$. \n",
    "\n",
    "_NB: this is a very bad choice of a polynomial basis to expand the function (most notably, because the different polynomials are not orthogonal). We are just doing this as a simple example, never try this for a real problem!_\n",
    "\n",
    "Play around with the widget below. It is _really_ difficult to fit the model manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "np.random.seed(12345)\n",
    "pr_x = (np.random.uniform(size=20)-0.5)*10\n",
    "pr_y = (np.random.uniform(size=20)-0.5)*3\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "for k in range(len(pr_w)):\n",
    "    pr_y += pr_w[k]*pr_x**k\n",
    "    \n",
    "def pr_plot(ax, **w):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    lw = list(w.values())\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    fit_w = np.linalg.lstsq(pr_X, pr_y, rcond=None)[0]\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    for k in range(len(lw)):\n",
    "        yy += lw[k]*xx**k\n",
    "        my += lw[k]*pr_x**k\n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        \n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-my)**2)\n",
    "    ax.plot(pr_x, pr_y, 'b.', label=\"train data\")\n",
    "    ax.plot(xx, yy, 'r--', label=\"manual fit\")\n",
    "    ax.text(-4,-1,f'$\\ell = ${l:.3f}')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_ylim(min(pr_y)-1, max(pr_y)+1)\n",
    "wp_pr = WidgetPlot(pr_plot, WidgetParbox(\n",
    "    w_0=(1.0, -5.0, 5.0, 0.01,  r'$w_0$'),\n",
    "    w_1=(0.01, -2.0, 2.0, 0.01, r'$w_1$'),\n",
    "    w_2=(0.01, -1.0, 1.0, 0.01, r'$w_2$'),\n",
    "    w_3=(-0.2, -1.0, 1.0, 0.01, r'$w_3$'),\n",
    "    w_4=(0.01, -0.1, 0.1, 0.01, r'$w_4$'),\n",
    "    w_5=(0.01, -0.1, 0.1, 0.01, r'$w_5$')\n",
    "))\n",
    "display(wp_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can be written in a vectorial form, $\\ell \\propto \\sum_i | \\mathbf{w}\\cdot\\mathbf{x}_i - y_i |^2$. If $\\mathbf{X}$ is the matrix collecting the $x_i^k$ as rows and $\\mathbf{y}$ is the vector collecting the targets, a closed form solution for the weight vector can be derived as\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "We can now start looking to more realistic issues that arise in the context of regression models. For starters, data can contain a certain level of _noise_. This can be actual noise, or (often) hidden input features or relationships that cannot be captured by the simplified model. Second, a model that predicts the targets only for the data it had been trained on is of very little use: we want to be able to do real predictions!\n",
    "For this reason, it is customary to set aside a fraction of the available data that is not used to determine the weights that minimize $\\ell$. The error on this _test set_ is an indication of how good the model would be when predicting a new point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "def pr2_plot(ax, tgt, fit, noise, hidden, npoints):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(12345)\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*10\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    \n",
    "    fit_w = np.linalg.lstsq(pr_X[::2], pr_y[::2], rcond=None)[0]    \n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(ty)-1, max(ty)+1+noise/2)    \n",
    "    ax.text(0.1,0.15,f'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.05,f'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "wp_pr2 = WidgetPlot(pr2_plot, WidgetParbox(    \n",
    "    noise=(5.0, 0.1,10,0.1, 'Noise'),\n",
    "    hidden=(0.0, 0, 5,0.01, 'Hidden', {\"readout_format\" : \".2f\"}),\n",
    "    npoints=(20, 5, 100, 1, r'$n_\\mathrm{train}$'),\n",
    "    tgt=(False, r'Show true target'),\n",
    "    fit=(True, r'Show best fit'),\n",
    "))\n",
    "display(wp_pr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**02a** Compare the error on the train and the test sets. Which is typically higher? How do train and test errors change when the number of training points is changed from the lowest to the highest level?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2a_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2a-answer\", ex2a_txt, \"value\")\n",
    "display(ex2a_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**02b** How do the train and test loss change when the level of noise is increased? And how do they change when the level of hidden relationships is increased? Can you clearly distinguish the effect of noise and hidden terms? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2b_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex2b-answer\", ex2b_txt, \"value\")\n",
    "display(ex2b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tendency of achieving very low loss on the train set and a much larger test set error is a general phenomenon known as [_overfitting_](https://en.wikipedia.org/wiki/Overfitting). Overfitting is usually particularly bad when the train set size is small, and/or the model contains many parameters. Polynomial regression is notorious for overfitting. \n",
    "\n",
    "A common strategy to avoid overfitting is known as _regularization_. In broad terms, regularization implies penalizing solutions of the model that are too rapidly varying, and favouring those that are smoother, even at the cost of a slight increase of the train set error. In linear regression, the most common approach is to introduce a [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) term, that is to write the loss as \n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{n_\\mathrm{train}} \\sum_i | \\mathbf{w}\\cdot \\mathbf{x}_i - y_i | ^2 + \\lambda |\\mathbf{w}|^2.\n",
    "$$\n",
    "\n",
    "This expression (often referred to as $L^2$ regularized least-squares fit, or ridge regression) yields a closed-form solution for the weights, \n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X}+\\lambda \\mathbf{1})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This widget allows you to experiment with the effect of ridge regularization on the same polynomial fitting exercise. \n",
    "\n",
    "_NB: given that we are using a very poor basis, and different features span widely different scales, the underlying implementation is slightly more complicated, in that different weights are scaled differently before computing the Tikhonov term. This scaling is done so that a single parameter can be meaningfully used to control the regularity of the fit._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 5\n",
    "pr_w = [3, 1, 1, -0.3, -0.05, 0.01]\n",
    "\n",
    "def pr3_plot(ax, tgt, fit, noise, hidden, npoints, lam):    \n",
    "    \n",
    "    xx = np.linspace(-5, 5, 60)\n",
    "    yy = np.zeros(len(xx))\n",
    "    np.random.seed(54321)\n",
    "    xsz = 10\n",
    "    pr_x = (np.random.uniform(size=2*npoints)-0.5)*xsz\n",
    "    pr_X = np.vstack( [pr_x**k for k in range(6)]).T\n",
    "    pr_y = (np.random.uniform(size=len(pr_x))-0.5)*noise\n",
    "    for k in range(len(pr_w)):\n",
    "        pr_y += pr_w[k]*pr_x**k\n",
    "    pr_y += hidden*np.sin(pr_x*4)\n",
    "    wscale = np.asarray([1/(xsz*0.5)**k for k in range(npoly+1)])\n",
    "    fit_w = np.linalg.solve(\n",
    "        pr_X[::2].T@pr_X[::2]+\n",
    "        10**lam*(npoints//2)*np.diag(wscale**2), \n",
    "        pr_X[::2].T@pr_y[::2])\n",
    "    my = pr_x*0\n",
    "    ty = np.zeros(len(xx))\n",
    "    fy = np.zeros(len(xx))\n",
    "    pr_fy = np.zeros(len(pr_x))\n",
    "    for k in range(len(pr_w)):                \n",
    "        ty += pr_w[k]*xx**k\n",
    "        fy += fit_w[k]*xx**k\n",
    "        pr_fy += fit_w[k]*pr_x**k\n",
    "    ty += hidden*np.sin(xx*4)\n",
    "    \n",
    "    l = np.mean((pr_y-pr_fy)[::2]**2)\n",
    "    lte = np.mean((pr_y-pr_fy)[1::2]**2)\n",
    "    ax.plot(pr_x[::2], pr_y[::2], 'b.', label=\"train data\")\n",
    "    ax.plot(pr_x[1::2], pr_y[1::2], 'kx', label=\"test data\")    \n",
    "    \n",
    "    if tgt:\n",
    "        ax.plot(xx, ty, 'b:', label=\"true target\")\n",
    "    if fit:\n",
    "        ax.plot(xx, fy, 'b--', label=\"best fit\")\n",
    "    \n",
    "    ax.set_ylim(min(ty)-1, max(ty)+1+noise/2)    \n",
    "    ax.text(0.1,0.25,f'$\\mathrm{{RMSE}}_\\mathrm{{train}} = ${np.sqrt(l):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.15,f'$|\\mathbf{{w}}| = ${np.linalg.norm(wscale*fit_w):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.text(0.1,0.05,f'$\\mathrm{{RMSE}}_\\mathrm{{test}} = ${np.sqrt(lte):.3f}', transform=ax.transAxes, c='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "wp_pr3 = WidgetPlot(pr3_plot, WidgetParbox(    \n",
    "    noise=(5.0, 0.1,10,0.1, 'Noise'),\n",
    "    hidden=(0.0, 0, 5,0.01, 'Hidden', {\"readout_format\" : \".2f\"}),\n",
    "    npoints=(10, 5, 100, 1, r'$n_\\mathrm{train}$'),\n",
    "    tgt=(True, r'Show true target'),\n",
    "    fit=(True, r'Show best fit'),\n",
    "    lam = (-5.0,-5,5,0.1, r'$\\log_{10} \\lambda$')\n",
    "))\n",
    "display(wp_pr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**03a** Work with (noise, hidden, ntrain) = (5,0,10). What is the value of $\\lambda$ that minimizes the _test_ error?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3a_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3a-answer\", ex3a_txt, \"value\")\n",
    "display(ex3a_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**03b** Working with the same number of parameters, comment on the behavior of the best fit function and the various diagnostics as you vary the regularization away from the optimum value.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3b_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3b-answer\", ex3b_txt, \"value\")\n",
    "display(ex3b_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**03c** Increase the number of training points to 100. How does the behavior of ridge regression change? Is the same value of $\\lambda$ still optimal? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3c_txt = Textarea(\"Write the answer here\", layout=Layout(width=\"100%\"))\n",
    "data_dump.register_field(\"ex3c-answer\", ex3c_txt, \"value\")\n",
    "display(ex3c_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization $\\lambda$ is one of the so-called _hyperparameters_ (\"hypers\"), that tune the behavior of the model but are not directly optimized on the train set. In this case, the number of polynomial terms is another hyperparameter. Optimizing the hyperparameters on the test set is bad practice, because this amounts to _data leakage_, and makes the test error less representative of the true generalization capabilities of the model. \n",
    "\n",
    "We won't get into details, but consider that strategies to optimize the hyperparameters is to set aside a _validation_ set that is not used for training nor for testing, but just to tune the hyperparameter values, or to perform _cross validation_, that implies splitting the training set into train/validation parts, and repeating the exercise over multiple _folds_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprints and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any data-driven study of materials involves codifying the structure and composition of the materials being studied into a mathematical form that is suitable to be used as the input of the subsequent steps. Here we focus in particular on the definition of _fingerprints_, or _descriptors_ - a vector of numbers that are associated with each structure, assembled into a _feature vector_ $\\mathbf{x}_i$. \n",
    "\n",
    "In this module we are going to use a dataset of materials from the [materials project](https://materialsproject.org/). The dataset has been reformatted as an extended XYZ file, that can be read, as usual, with the ASE `read` function. The target properties (mostly related to the elastic behavior) can be read in the `info` dictionary of each frame.\n",
    "\n",
    "```\n",
    "data = read(\"filename.xyz\", \":\")\n",
    "X = []\n",
    "y = []\n",
    "for structure in data:\n",
    "    X.append(get_features(structure.symbols, structure.cell, structure.positions))\n",
    "    y.append(structure.info['property_name'])\n",
    "```\n",
    "\n",
    "where `get_features` is a function that processes the structural information of each frame to convert it into a set of fingerprints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptors can be either precise [_representations_](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00021) of the coordinates and chemical nature of all atoms in a structure (which are commonplace in the construction of machine-learning interatomic potentials) or fingerprints based on a combination of structural parameters and properties that can be computed easily. For instance, one could take the electronegativity of elements in combination with the point group of the crystal structure. \n",
    "\n",
    "Here we take a very simple (and somewhat crude) approach, describing each structure by its chemical composition - that is, the feature $x_{iZ}$ contains the fraction of the atoms of structure $i$ that has atomic number $Z$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**04** Write a function that takes structural information for each frame and returns a vector containing the fractional composition of each compound, to be used as descriptors. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVYA: get them to write a function that loads the structures and \n",
    "# computes the vectors with the fractional composition. Demo should be a table with the symbols string and the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVYA: get them to write a function that loads the structures, computes the vectors with the fractional composition.\n",
    "# it should take as an argument the maximum polynomial order, and also compute the powers of the fractional composition,\n",
    "# stacking them to form a larger feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be just a \"custom\" function where they can play around and compute their own fingerprints based on the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "461px",
    "left": "0px",
    "right": "927.667px",
    "top": "107px",
    "width": "139px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
